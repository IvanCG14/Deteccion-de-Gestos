# Sistema de Reconocimiento de Gestos Multimodal

Sistema de clasificaci√≥n de gestos de mano (Rock, Paper, Scissors, None) usando Deep Learning multimodal que fusiona im√°genes RGB y landmarks 3D de MediaPipe, complementado con datos del brazalete Myo Armband.

## üìã Tabla de Contenidos

- [Descripci√≥n](#descripci√≥n)
- [Requisitos del Sistema](#requisitos-del-sistema)
- [Scripts de Utilidad](#scripts-de-utilidad)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Entrenamiento](#entrenamiento)
- [Arquitectura del Modelo](#arquitectura-del-modelo)
- [Troubleshooting](#troubleshooting)
---

## Descripci√≥n

Este proyecto implementa un modelo de deep learning multimodal para reconocimiento de gestos de mano. Combina:
- **Modalidad Visual (RGB)**: Im√°genes procesadas con ResNet-18 preentrenado
- **Modalidad Esquel√©tica (3D)**: 21 landmarks de MediaPipe procesados con MLP

El sistema alcanza **100% de accuracy** en el conjunto de test con 4 clases de gestos.

---

## Requisitos del Sistema

#### Hardware
- **M√≠nimo**: CPU (funcional pero lento ~2 min/epoch)
- **Recomendado**: GPU NVIDIA con CUDA (10x m√°s r√°pido)
- **RAM**: 8GB m√≠nimo, 16GB recomendado
- **Almacenamiento**: ~2GB para dataset + modelos

#### Software
- **Sistema Operativo**: Windows 10/11, Linux, macOS
- **Python**: 3.8 - 3.11 (recomendado 3.10)
- **CUDA** (opcional): 11.8+ para aceleraci√≥n GPU

#### üìÑ requirements.txt

```txt
# Core Deep Learning
torch==2.9.1  # PyTorch framework
torchvision==0.24.1  # Modelos preentrenados y transforms
numpy==1.23.5  # Operaciones num√©ricas

# Data Processing
pandas==2.2.3  # Manipulaci√≥n de CSVs
pillow==11.1.0  # Procesamiento de im√°genes (PIL)
opencv-python==4.11.0.86  # Computer vision (opcional, para MediaPipe)

# Machine Learning Utilities
scikit-learn==1.6.1  # Metrics, train_test_split
scipy==1.15.3  # Operaciones cient√≠ficas

# Visualization
matplotlib==3.10.0  # Gr√°ficas de entrenamiento
seaborn==0.13.2  # Matriz de confusi√≥n (heatmaps)

# Progress Bars
tqdm==4.67.1  # Barras de progreso durante entrenamiento
```

---

## Scripts de Utilidad

Dentro de la carpeta ```utils```, se encuentran herramientas dise√±adas para pruebas de hardware, validaci√≥n de visi√≥n por computadora y preparaci√≥n de datos:

**1. ```funtion_karen.py``` (Prueba de Myo Armband):**

- **Prop√≥sito:** Verificar la conexi√≥n con el brazalete Myo y asegurar que el sistema est√© recibiendo correctamente las se√±ales de EMG (electromiograf√≠a) e IMU (inerciales).
- **Uso:** Ejecutar para diagnosticar problemas de conectividad o latencia con el sensor Myo.

**2. ```manos.py``` (Detecci√≥n de Landmarks):**

- **Prop√≥sito:** Validar que la c√°mara sea reconocida correctamente y que el modelo de MediaPipe est√© detectando los 21 puntos clave (landmarks) de la mano de forma fluida.
- **Uso:** Ideal para calibrar la iluminaci√≥n y el encuadre de la c√°mara antes de capturar datos.

**3. ```rsp.py``` (L√≥gica Base Piedra, Papel o Tijera):**

- **Prop√≥sito:** Implementa la l√≥gica algor√≠tmica inicial para reconocer los gestos de Piedra, Papel y Tijera bas√°ndose en la posici√≥n de los dedos.
- **Importancia:** Este c√≥digo sirve como motor fundamental para el sistema de etiquetado autom√°tico y la creaci√≥n de nuevos datasets para el entrenamiento del modelo neuronal.

---

## Estructura del Proyecto

```
modelo/
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ funtion_karen.py   # Diagn√≥stico de datos Myo
‚îÇ   ‚îú‚îÄ‚îÄ manos.py           # Test de detecci√≥n de c√°mara
‚îÇ   ‚îî‚îÄ‚îÄ rsp.py             # L√≥gica base de gestos y recolecci√≥n
‚îú‚îÄ‚îÄ model_training.ipynb   # Notebook de entrenamiento
‚îú‚îÄ‚îÄ multimodal_myo_model.py # Definici√≥n de la arquitectura del modelo
‚îú‚îÄ‚îÄ myo_dataset_explore.ipynb # An√°lisis exploratorio de datos
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md              # Documentaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ dataset/               # Dataset de gestos, debe estar al mismo nivel que el archivo de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ images/                 # Im√°genes (Rock/Paper/Scissors/None)
‚îÇ   ‚îú‚îÄ‚îÄ landmarks/              # CSV de puntos clave y archivos JSON de metadata
‚îÇ   ‚îú‚îÄ‚îÄ emg/                    # CSV con los 8 canales de se√±ales musculares (solo Myo)
‚îÇ   ‚îî‚îÄ‚îÄ imu/                    # CSV con orientaci√≥n y aceleraci√≥n (solo Myo)
‚îÇ
‚îú‚îÄ‚îÄ best_model_synchronized.pth               # Mejor modelo guardado
‚îú‚îÄ‚îÄ results_synchronized.png                  # Gr√°ficas de loss/accuracy
‚îî‚îÄ‚îÄ confusion_matrix.png         # Matriz de confusi√≥n

```

---

## Entrenamiento

Esta secci√≥n detalla el flujo de trabajo para entrenar el modelo de clasificaci√≥n de gestos utilizando fusi√≥n multimodal (Visi√≥n + Biose√±ales).

#### 1. Preparar el Dataset
El script de entrenamiento requiere una estructura de datos sincronizada. Aseg√∫rate de que tu carpeta `dataset/` contenga el archivo maestro de metadatos:

* **Im√°genes:** Fotos en formato `.jpg` organizadas por clase.
* **EMG/IMU:** Archivos `.csv` con las se√±ales del brazalete.

#### 2. Configuraci√≥n del Notebook
En el archivo `model_training.ipynb`, el entrenamiento est√° configurado con los siguientes par√°metros por defecto:

* **Arquitectura:** Red neuronal h√≠brida (CNN para im√°genes + MLP para sensores).
* **Resoluci√≥n de Imagen:** `128x128` p√≠xeles.
* **Batch Size:** `16`
* **√âpocas:** `50` (con guardado autom√°tico del mejor modelo).
* **Optimizador:** Adam con `Learning Rate = 1e-4` y scheduler de coseno.

#### 3. Ejecuci√≥n
Para iniciar el proceso en tu entorno de Jupyter:

1. Activa tu environment.
2. Dir√≠gete a la carpeta con el comando `cd ../../Deteccion-de-Gestos/modelo`
3. Ejecuta `jupyter notebook`
4. Abre el notebook `model_training.ipynb`
5. Ejecuta la celda de carga de datos para verificar que el `base_path` sea correcto.
6. Inicia el entrenamiento ejecutando la celda principal (`train()`). 

El script dividir√° autom√°ticamente tus muestras en:
- **80%** Entrenamiento.
- **10%** Validaci√≥n (para control de sobreajuste).
- **10%** Test (evaluaci√≥n final).

#### 4. Salida de Resultados
Una vez finalizado el entrenamiento, el sistema genera autom√°ticamente dos archivos en la ra√≠z del proyecto:

1.  **`best_model_synchronized.pth`**: El archivo con los pesos del modelo que obtuvo el mejor desempe√±o en validaci√≥n.
2.  **`results_synchronized.png`**: Un panel gr√°fico que resume todo el entrenamiento.



#### 5. Verificaci√≥n de M√©tricas
Al final del entrenamiento, el notebook despliega un reporte detallado. Puedes verificar la precisi√≥n por clase (Precision, Recall y F1-Score) para confirmar que el modelo no tiene sesgos:

<p align="center">
  <img src="imgs/results_synchronized01.png" alt="Ejemplo de resultados" width="600">
  <br>
  <em>Ejemplo de resultados</em>
</p>

---

## Arquitectura del Modelo

El sistema utiliza una arquitectura de fusi√≥n multimodal dise√±ada para procesar diferentes tipos de se√±ales en paralelo antes de combinarlas para la clasificaci√≥n final:

<p align="center">
  <img src="imgs/diagrama_arquitectura.png" alt="Diagrama de la Arquitectura del Modelo" width="600">
  <br>
  <em>Diagrama de flujo de datos y fusi√≥n de modalidades</em>
</p>

#### Componentes Principales:
* **Rama RGB:** Utiliza una **ResNet-18** preentrenada como extractor de caracter√≠sticas para capturar la informaci√≥n espacial y visual del gesto desde la c√°mara.
* **Rama Landmarks:** Un Perceptr√≥n Multicapa (**MLP**) que procesa las 21 coordenadas 3D (x, y, z) de la mano proporcionadas por MediaPipe.
* **Integraci√≥n Myo:** Estructura preparada para procesar se√±ales temporales de **EMG e IMU** a trav√©s de capas recurrentes (LSTM), permitiendo una clasificaci√≥n robusta incluso en condiciones de baja iluminaci√≥n.
* **Capa de Fusi√≥n:** Las caracter√≠sticas de todas las ramas se concatenan y pasan por capas densas finales para predecir la probabilidad de cada gesto (Piedra, Papel, Tijera o Ninguno).

### üñºÔ∏è Data Augmentation

Para evitar el sobreajuste (overfitting) y mejorar la robustez del modelo multimodal, aplicamos t√©cnicas de aumento de datos diferenciadas por rama y una estrategia de mezcla global (**Mixup**).

#### Aumento de Datos por Rama

| Rama | T√©cnica Aplicada | Prop√≥sito del Augmentation |
| :--- | :--- | :--- |
| **Visual (RGB)** | `RandomRotation` + `ColorJitter` | Simula variaciones en el √°ngulo de la c√°mara y cambios dr√°sticos de iluminaci√≥n. |
| **Visual (RGB)** | `RandomHorizontalFlip` | Permite que el modelo reconozca gestos tanto de la mano derecha como de la izquierda. |
| **Landmarks** | `Gaussian Noise` (Opcional) | A√±ade peque√±as variaciones a las coordenadas (x, y) para tolerar errores de detecci√≥n de MediaPipe. |
| **Sensores (EMG/IMU)** | `Time Shifting` | Desplaza ligeramente la ventana de tiempo de las se√±ales para que el modelo no dependa de un inicio exacto del gesto. |

### üîÄ Estrategia Mixup (Fusi√≥n de Muestras)

El notebook implementa Mixup, una t√©cnica de regularizaci√≥n que combina dos muestras aleatorias del dataset durante el entrenamiento para crear una "muestra sint√©tica".

**¬øC√≥mo funciona?** Si tenemos una muestra de "Piedra" y otra de "Papel", el Mixup crea una imagen y unas se√±ales de sensores que son una combinaci√≥n lineal de ambas (por ejemplo, 70% Piedra y 30% Papel).

**Beneficios para este proyecto:**

1. **Suavizado de fronteras:** Obliga al modelo a no ser "demasiado seguro" de sus predicciones, lo que mejora la generalizaci√≥n.
2. **Robustez Multimodal**: Ayuda a que las ramas de sensores y visi√≥n se alineen incluso cuando las se√±ales son ruidosas.
3. **Estabilidad:** Reduce significativamente las oscilaciones en la curva de p√©rdida (Loss) durante las √∫ltimas √©pocas.

**Nota:** El Mixup solo se aplica durante el **entrenamiento**. Para la validaci√≥n y el test, las muestras se mantienen puras para obtener una evaluaci√≥n real del desempe√±o.

---

## Troubleshooting

#### Problema 1: `RuntimeError: Expected more than 1 value per channel when training`

**Causa:** Batch de tama√±o 1 con BatchNorm en modo training.

**Soluci√≥n:**
```python
# En create_dataloaders(), a√±adir drop_last=True
train_loader = DataLoader(..., drop_last=True)
```

#### Problema 2: `CUDA out of memory`

**Causa:** GPU sin memoria suficiente.

**Soluciones:**
1. Reducir `batch_size` (de 16 a 8 o 4)
2. Usar CPU: `CONFIG['device'] = 'cpu'`
3. Usar modelo m√°s peque√±o (cambiar ResNet-18 por MobileNetV3)

#### Problema 3: `ModuleNotFoundError: No module named 'torch'`

**Causa:** Dependencias no instaladas.

**Soluci√≥n:**
```bash
pip install torch torchvision
# Para GPU seg√∫n tu versi√≥n de CUDA
# Ej: para versi√≥n CUDA 11.8:
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

#### Problema 4: Accuracy no mejora (se queda en ~25%)

**Causa:** Modelo no est√° aprendiendo.

**Posibles soluciones:**
1. Verificar labels en CSV (deben coincidir con carpetas)
2. Aumentar learning rate: `1e-3`
3. Desactivar pesos de clase temporalmente
4. Revisar normalizaci√≥n de landmarks

#### Problema 5: Overfitting (Train acc=100%, Val acc=60%)

**Causa:** Modelo memoriza train set.

**Soluciones:**
1. Aumentar dropout: `0.5`
2. M√°s data augmentation
3. Early stopping m√°s agresivo
4. Reducir complejidad del modelo

---

## Contribuciones

Las contribuciones son bienvenidas. Para cambios grandes:
1. Fork el repositorio
2. Crea un branch (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add AmazingFeature'`)
4. Push al branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

