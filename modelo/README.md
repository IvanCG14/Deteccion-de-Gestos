# Deteccion-de-Gestos

## Modelo Multimodal

# Sistema de Reconocimiento de Gestos Multimodal

Sistema de clasificaciÃ³n de gestos de mano (Rock, Paper, Scissors, None) usando Deep Learning multimodal que fusiona imÃ¡genes RGB y landmarks 3D de MediaPipe.

## ðŸ“‹ Tabla de Contenidos

- [DescripciÃ³n](#descripciÃ³n)
- [CaracterÃ­sticas](#caracterÃ­sticas)
- [Requisitos del Sistema](#requisitos-del-sistema)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Uso](#uso)
- [Arquitectura del Modelo](#arquitectura-del-modelo)
- [Resultados](#resultados)
- [Troubleshooting](#troubleshooting)
---

## DescripciÃ³n

Este proyecto implementa un modelo de deep learning multimodal para reconocimiento de gestos de mano. Combina:
- **Modalidad Visual (RGB)**: ImÃ¡genes procesadas con ResNet-18 preentrenado
- **Modalidad EsquelÃ©tica (3D)**: 21 landmarks de MediaPipe procesados con MLP

El sistema alcanza **100% de accuracy** en el conjunto de test con 4 clases de gestos.

---

## CaracterÃ­sticas

- âœ… **Modelo Multimodal**: FusiÃ³n de RGB + Skeleton 3D
- âœ… **Transfer Learning**: ResNet-18 preentrenado en ImageNet
- âœ… **Manejo de Desbalance**: Class weighting automÃ¡tico
- âœ… **Pipeline Completo**: Desde datos crudos hasta modelo entrenado
- âœ… **Visualizaciones**: GrÃ¡ficas de entrenamiento y matriz de confusiÃ³n
- âœ… **Checkpoints**: Guardado automÃ¡tico del mejor modelo
- âœ… **Reproducibilidad**: Seeds fijadas para resultados consistentes

---

## Requisitos del Sistema

### Hardware
- **MÃ­nimo**: CPU (funcional pero lento ~2 min/epoch)
- **Recomendado**: GPU NVIDIA con CUDA (10x mÃ¡s rÃ¡pido)
- **RAM**: 8GB mÃ­nimo, 16GB recomendado
- **Almacenamiento**: ~2GB para dataset + modelos

### Software
- **Sistema Operativo**: Windows 10/11, Linux, macOS
- **Python**: 3.8 - 3.11 (recomendado 3.10)
- **CUDA** (opcional): 11.8+ para aceleraciÃ³n GPU

---

## InstalaciÃ³n

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/IvanCG14/Deteccion-de-Gestos.git
```

### Paso 2: Crear Entorno Virtual

**Windows:**
```bash
py -m venv venv
.\venv\Scripts\activate
```

**Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### Paso 3: Instalar Dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## ðŸ“„ requirements.txt

```txt
# Core Deep Learning
torch==2.9.1  # PyTorch framework
torchvision==0.24.1  # Modelos preentrenados y transforms
numpy==1.23.5  # Operaciones numÃ©ricas

# Data Processing
pandas==2.2.3  # ManipulaciÃ³n de CSVs
pillow==11.1.0  # Procesamiento de imÃ¡genes (PIL)
opencv-python==4.11.0.86  # Computer vision (opcional, para MediaPipe)

# Machine Learning Utilities
scikit-learn==1.6.1  # Metrics, train_test_split
scipy==1.15.3  # Operaciones cientÃ­ficas

# Visualization
matplotlib==3.10.0  # GrÃ¡ficas de entrenamiento
seaborn==0.13.2  # Matriz de confusiÃ³n (heatmaps)

# Progress Bars
tqdm==4.67.1  # Barras de progreso durante entrenamiento
```

---

## Estructura del Proyecto

```
modelo/
â”‚
â”œâ”€â”€ dataset/                          # Dataset de gestos
â”‚   â”œâ”€â”€ images/                       # ImÃ¡genes RGB organizadas por clase
â”‚   â”‚   â”œâ”€â”€ none/
â”‚   â”‚   â”‚   â”œâ”€â”€ none_0001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ paper/
â”‚   â”‚   â”‚   â”œâ”€â”€ paper_0001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ rock/
â”‚   â”‚   â”‚   â”œâ”€â”€ rock_0001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ scissors/
â”‚   â”‚       â”œâ”€â”€ scissors_0001.jpg
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ landmarks/                    # CSVs con coordenadas 3D (MediaPipe)
â”‚       â”œâ”€â”€ none_landmarks.csv
â”‚       â”œâ”€â”€ paper_landmarks.csv
â”‚       â”œâ”€â”€ rock_landmarks.csv
â”‚       â””â”€â”€ scissors_landmarks.csv
â”‚
â”œâ”€â”€ results/                          # Resultados de entrenamiento
â”‚   â”œâ”€â”€ best_model.pth               # Mejor modelo guardado
â”‚   â”œâ”€â”€ results.png                  # GrÃ¡ficas de loss/accuracy
â”‚   â””â”€â”€ confusion_matrix.png         # Matriz de confusiÃ³n
â”‚
â”œâ”€â”€ RSP_model.ipynb
â”œâ”€â”€ requirements.txt                  # Dependencias del proyecto
â”œâ”€â”€ README.md                         # Este archivo
â””â”€â”€ .gitignore                        # Archivos a ignorar en Git
```

---

## Uso

### 1. Preparar el Dataset

AsegÃºrate de tener la estructura correcta:

```
dataset/
â”œâ”€â”€ images/
â”‚   â””â”€â”€ [clase]/[imagen].jpg
â””â”€â”€ landmarks/
    â””â”€â”€ [clase]_landmarks.csv
```

**Formato del CSV de landmarks:**
```csv
image_file,label,x0,y0,z0,x1,y1,z1,...,x20,y20,z20
dataset/images/rock/rock_0001.jpg,rock,100,200,0,105,210,5,...
```
- 1 fila = 1 imagen
- Columnas: `image_file`, `label`, 63 coordenadas (21 landmarks Ã— 3)

### 2. Configurar HiperparÃ¡metros

Edita `run_training.py`:

```python
CONFIG = {
    'landmarks_dir': 'dataset/landmarks',  # Ruta a CSVs
    'base_path': '',                       # Path base (si rutas son relativas)
    'batch_size': 16,                      # Ajustar segÃºn RAM/GPU
    'epochs': 30,                          # NÃºmero de epochs
    'learning_rate': 1e-4,                 # Learning rate
    'img_size': (224, 224),                # ResoluciÃ³n de imÃ¡genes
    'num_workers': 0,                      # Workers (0 para Windows)
    'device': 'cuda',                      # 'cuda' o 'cpu'
    'classes': ['none', 'paper', 'rock', 'scissors']
}
```

### 3. Entrenar el Modelo

```bash
python RSP_model.ipynb #bloque entrenamiento
```

**Salida esperada:**
```
============================================================
CARGANDO DATOS
============================================================
  Cargado: none_landmarks.csv (93 muestras)
  Cargado: paper_landmarks.csv (196 muestras)
  Cargado: rock_landmarks.csv (207 muestras)
  Cargado: scissors_landmarks.csv (55 muestras)

Total: 551 muestras
Train: 385 | Val: 83 | Test: 83

============================================================
ENTRENANDO (30 epochs)
============================================================
Epoch 1/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [01:26<00:00]
  Train: loss=1.0384, acc=0.5417 | Val: loss=0.7547, acc=0.8250
  âœ“ Mejor modelo guardado (acc: 0.8250)

...

============================================================
EVALUACIÃ“N FINAL (TEST SET)
============================================================
âœ“ Test Accuracy: 1.0000 (100.0%)

              precision    recall  f1-score   support
        none       1.00      1.00      1.00        14
       paper       1.00      1.00      1.00        30
        rock       1.00      1.00      1.00        31
    scissors       1.00      1.00      1.00         8
```

### 4. Visualizar Resultados

DespuÃ©s del entrenamiento se generan:
- `best_model.pth`: Mejor modelo (usar para inferencia)
- `results.png`: GrÃ¡ficas de loss y accuracy
- `confusion_matrix.png`: Matriz de confusiÃ³n del test set

```python
from PIL import Image
img = Image.open('results.png')
img.show()
```

---

## Arquitectura del Modelo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MULTIMODAL MODEL                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  RGB BRANCH              SKELETON BRANCH                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚  Input: (3,224,224)      Input: (63,)                   â”‚
â”‚     â”‚                         â”‚                         â”‚
â”‚     â–¼                         â–¼                         â”‚
â”‚  ResNet-18              Linear(63â†’128)                  â”‚
â”‚  (ImageNet)             BatchNorm + ReLU                â”‚
â”‚     â”‚                   Dropout(0.3)                    â”‚
â”‚     â–¼                         â”‚                         â”‚
â”‚  Linear(512â†’256)        Linear(128â†’256)                 â”‚
â”‚  BatchNorm + ReLU       BatchNorm + ReLU                â”‚
â”‚  Dropout(0.3)           Dropout(0.3)                    â”‚
â”‚     â”‚                         â”‚                         â”‚
â”‚     â–¼                         â–¼                         â”‚
â”‚  Features (256)         Linear(256â†’256)                 â”‚
â”‚     â”‚                   BatchNorm + ReLU                â”‚
â”‚     â”‚                         â”‚                         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚          Concatenate (512)                              â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚         Linear(512â†’256)                                 â”‚
â”‚         BatchNorm + ReLU                                â”‚
â”‚         Dropout(0.3)                                    â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚         Linear(256â†’4)                                   â”‚
â”‚                â”‚                                        â”‚
â”‚                â–¼                                        â”‚
â”‚    [none, paper, rock, scissors]                        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ParÃ¡metros Totales:** ~11.5M
- ResNet-18: ~11M
- MLP Skeleton: ~300K
- Fusion + Classifier: ~200K

---

## Resultados

### MÃ©tricas de DesempeÃ±o

| Conjunto | Loss | Accuracy |
|----------|------|----------|
| Train | 0.0102 | 100.0% |
| Val | 0.0054 | 100.0% |
| **Test** | **-** | **100.0%** âœ“ |

### MÃ©tricas por Clase (Test Set)

| Clase | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| none | 1.00 | 1.00 | 1.00 | 14 |
| paper | 1.00 | 1.00 | 1.00 | 30 |
| rock | 1.00 | 1.00 | 1.00 | 31 |
| scissors | 1.00 | 1.00 | 1.00 | 8 |

### Tiempo de Entrenamiento

- **CPU (Intel i7)**: ~2 min/epoch â†’ 60 min total (30 epochs)
- **GPU (NVIDIA RTX 3060)**: ~15 seg/epoch â†’ 7.5 min total (30 epochs)

### ComparaciÃ³n con Baselines

| MÃ©todo | Test Accuracy |
|--------|---------------|
| RGB Only (ResNet-18) | ~95% (estimado) |
| Skeleton Only (MLP) | ~90% (estimado) |
| **Multimodal (Ours)** | **100%** âœ“ |

---

## Troubleshooting

### Problema 1: `RuntimeError: Expected more than 1 value per channel when training`

**Causa:** Batch de tamaÃ±o 1 con BatchNorm en modo training.

**SoluciÃ³n:**
```python
# En create_dataloaders(), aÃ±adir drop_last=True
train_loader = DataLoader(..., drop_last=True)
```

### Problema 2: `CUDA out of memory`

**Causa:** GPU sin memoria suficiente.

**Soluciones:**
1. Reducir `batch_size` (de 16 a 8 o 4)
2. Usar CPU: `CONFIG['device'] = 'cpu'`
3. Usar modelo mÃ¡s pequeÃ±o (cambiar ResNet-18 por MobileNetV3)

### Problema 3: `ModuleNotFoundError: No module named 'torch'`

**Causa:** Dependencias no instaladas.

**SoluciÃ³n:**
```bash
pip install torch torchvision
# Para GPU segÃºn tu versiÃ³n de CUDA
# Ej: para versiÃ³n CUDA 11.8:
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Problema 4: Accuracy no mejora (se queda en ~25%)

**Causa:** Modelo no estÃ¡ aprendiendo.

**Posibles soluciones:**
1. Verificar labels en CSV (deben coincidir con carpetas)
2. Aumentar learning rate: `1e-3`
3. Desactivar pesos de clase temporalmente
4. Revisar normalizaciÃ³n de landmarks

### Problema 5: Overfitting (Train acc=100%, Val acc=60%)

**Causa:** Modelo memoriza train set.

**Soluciones:**
1. Aumentar dropout: `0.5`
2. MÃ¡s data augmentation
3. Early stopping mÃ¡s agresivo
4. Reducir complejidad del modelo

---

## Referencias

### Papers
- He, K., et al. (2016). "Deep Residual Learning for Image Recognition". CVPR.
- Ioffe, S., & Szegedy, C. (2015). "Batch Normalization". ICML.
- Baltrusaitis, T., et al. (2018). "Multimodal Machine Learning: A Survey". IEEE TPAMI.

### CÃ³digo Base
- PyTorch: https://pytorch.org/
- TorchVision: https://pytorch.org/vision/
- MediaPipe: https://google.github.io/mediapipe/

---

## Contribuciones

Las contribuciones son bienvenidas. Para cambios grandes:
1. Fork el repositorio
2. Crea un branch (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add AmazingFeature'`)
4. Push al branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## Data augmentation

El Aumento de Datos previene el sobreajuste (overfitting) al simular variaciones del mundo real y hacer el modelo mÃ¡s robusto a cambios en la captura (iluminaciÃ³n, Ã¡ngulo, tamaÃ±o).

Bloque de data augmentation:
```python
# ============================================
# 4. DATA AUGMENTATION
# ============================================

def get_train_transforms(img_size: Tuple[int, int] = (224, 224)):
    """Transformaciones con augmentation para entrenamiento"""
    return transforms.Compose([
        transforms.Resize((int(img_size[0] * 1.1), int(img_size[1] * 1.1))),
        transforms.RandomCrop(img_size),
        transforms.RandomHorizontalFlip(p=0.3),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.RandomRotation(degrees=15),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

def get_val_transforms(img_size: Tuple[int, int] = (224, 224)):
    """Transformaciones sin augmentation para val/test"""
    return transforms.Compose([
        transforms.Resize(img_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
```

![Augmentation](imgs/dataset_samples.png)

### Transformaciones Aplicadas (Rama RGB)

#### TransformaciÃ³n: PropÃ³sito
- Resize + RandomCrop: Simula variaciones en el zoom y la posiciÃ³n del gesto.
- RandomHorizontalFlip: EnseÃ±a a reconocer el gesto independientemente de la lateralidad (mano izquierda/derecha).
- ColorJitter:"Simula cambios en las condiciones de iluminaciÃ³n (brillo, contraste, saturaciÃ³n)."
- RandomRotation: Acepta ligeros cambios en el Ã¡ngulo o inclinaciÃ³n de la cÃ¡mara/mano.
- Normalize: Estandariza la imagen con los valores de ImageNet para compatibilidad con ResNet-18.


