# Detecci√≥n de Gestos Multimodal (Rock, Paper, Scissors)

Este proyecto implementa un sistema de reconocimiento de gestos mediante **Deep Learning Multimodal**, fusionando visi√≥n artificial (RGB + Landmarks) y se√±ales bioel√©ctricas (EMG + IMU) del brazalete **MYO Armband**.

---

## üõ†Ô∏è 1. Instalaci√≥n del Entorno

Sigue estos pasos para configurar tu ambiente de desarrollo con soporte para GPU.

### Preparaci√≥n Inicial

```bash
# en la carpeta de preferencia
git clone https://github.com/IvanCG14/Deteccion-de-Gestos.git
```

Navega al directorio `environment` que contiene los archivos de configuraci√≥n:

```bash
cd ./environment/
```

Los archivos `environment.yml` y `requirements.txt` permiten reproducir el entorno completo.

### Opci√≥n A: Instalaci√≥n autom√°tica (Recomendada)

Usa el fichero `environment.yml` para crear el entorno con todas las dependencias especificadas.

```bash
conda env create -f environment.yml
conda activate cti_env_gpu
```

### Opci√≥n B: Instalaci√≥n manual (Paso a paso)

```bash
# 1. Crear entorno
conda create -n cti_env_gpu python=3.11.14 pip -y
conda activate cti_env_gpu

# 2. Instalar PyTorch con soporte CUDA 13.0
pip install torch==2.9.0+cu130 torchvision==0.24.0+cu130 --index-url [https://download.pytorch.org/whl/cu130](https://download.pytorch.org/whl/cu130)

# 3. Instalar dependencias del proyecto
pip install -r requirements.txt
```

### Verificaci√≥n de Hardware:

```bash
# Comprobar estado de la GPU
nvidia-smi

# Verificar PyTorch en Python
python -c "import torch; print(f'Torch: {torch.__version__} | CUDA: {torch.cuda.is_available()}')"
```

---

## üì∏ 2. Generaci√≥n de Datasets

En la carpeta `getdata/` se encuentran las herramientas necesarias para construir el dataset, permitiendo elegir entre un flujo de trabajo puramente visual o uno multimodal avanzado.

### 1. Dataset de 2 Modalidades (B√°sico)
Utiliza el script `getdata_rsp.py` para capturas basadas √∫nicamente en visi√≥n artificial.
* **Ramas:** Imagen RGB y Marcadores 3D (Landmarks).
* **Tecnolog√≠as:** OpenCV y MediaPipe.
* **Uso:** Ideal para modelos que no requieren sensores externos.

### 2. Dataset de 4 Ramas (Multimodal - Myo Armband)
Utiliza el script `dataset_creator_myo.py` para una captura completa y sincronizada de biose√±ales y visi√≥n.
* **Ramas:**
    1.  **RGB:** Im√°genes de alta definici√≥n.
    2.  **Marcadores 3D:** Coordenadas espaciales de la mano.
    3.  **EMG:** 8 canales de actividad el√©ctrica muscular.
    4.  **IMU:** Datos inerciales (orientaci√≥n, aceleraci√≥n y giroscopio).
* **Sincronizaci√≥n:** El script gestiona hilos independientes para asegurar que los datos de los sensores coincidan exactamente con el frame capturado por la c√°mara, generando un archivo `metadata.json` como √≠ndice maestro.

### üìÇ Recursos y Referencias
* **Scripts de captura:** [Carpeta getdata/](getdata/)
* **Dataset de ejemplo:** [Dataset Multimodal Sincronizado](https://1drv.ms/f/c/66c04837d2873fa4/IgCSyiKERBCESYB2pku-jSTYAdretsgtq320lxWYOVtWO4M?e=l4WRRh)

> **Nota:** Para el uso del sistema de 4 ramas, aseg√∫rate de tener el SDK de Myo y el brazalete correctamente calibrado en el antebrazo.

---

## üß† 3. Modelo Multimodal

Este proyecto implementa una arquitectura de **Deep Learning Multimodal** dise√±ada para el reconocimiento de gestos en tiempo real. El modelo utiliza una estrategia de **Fusi√≥n Tard√≠a (Late Fusion)**, donde cada modalidad es procesada por una rama especializada antes de combinarse en una capa de clasificaci√≥n com√∫n.

### Arquitectura de 4 Ramas
Basado en el n√∫cleo de `model_training.ipynb`, el sistema integra:

* **Rama Visual (CNN):** Utiliza una **ResNet-18** (Transfer Learning) para extraer caracter√≠sticas espaciales de im√°genes RGB redimensionadas a `128x128`.
* **Rama Esquel√©tica (3D):** Un bloque de capas densas (MLP) que procesa los 21 landmarks (63 coordenadas) extra√≠dos por MediaPipe.
* **Rama EMG (Biose√±ales):** Procesa los 8 canales de electromiograf√≠a del brazalete Myo para detectar la intensidad de la contracci√≥n muscular.
* **Rama IMU (Inercial):** Analiza la orientaci√≥n (cuaterniones), aceleraci√≥n y velocidad angular para capturar la din√°mica del movimiento.

### Caracter√≠sticas Principales

- üöÄ **Fusi√≥n Sincronizada**: El modelo procesa muestras donde la imagen y las se√±ales de los sensores ocurren en la misma ventana temporal mediante el archivo `metadata.json`.
- üß¨ **Regularizaci√≥n con Mixup**: Implementa aumento de datos por mezcla lineal de muestras, lo que mejora dr√°sticamente la generalizaci√≥n y reduce el overfitting.
- ‚öñÔ∏è **Optimizaci√≥n Avanzada**: Uso de `CosineAnnealingLR` para un decaimiento suave de la tasa de aprendizaje y `Adam` como optimizador.
- üìä **Evaluaci√≥n Exhaustiva**: Generaci√≥n autom√°tica de matrices de confusi√≥n y reportes de clasificaci√≥n (Precision, Recall, F1) para cada gesto.
- üíæ **Gesti√≥n de Checkpoints**: El sistema monitorea el *Validation Loss* y guarda autom√°ticamente el estado √≥ptimo en `best_model_synchronized.pth`.

---

### Requisitos del Sistema

#### Hardware
- **M√≠nimo**: CPU (funcional pero lento ~2 min/epoch)
- **Recomendado**: GPU NVIDIA con CUDA (10x m√°s r√°pido)
- **RAM**: 8GB m√≠nimo, 16GB recomendado
- **Almacenamiento**: ~2GB para dataset + modelos

#### Software
- **Sistema Operativo**: Windows 10/11, Linux, macOS
- **Python**: 3.8 - 3.11 (recomendado 3.10)
- **CUDA** (opcional): 11.8+ para aceleraci√≥n GPU

---

## Referencias

### Papers
- 

---

üìß Contacto e Investigaci√≥n
Proyecto desarrollado para la investigaci√≥n en interfaces hombre-m√°quina y fusi√≥n sensorial. 
**Licencia:** Open Source para fines educativos.



