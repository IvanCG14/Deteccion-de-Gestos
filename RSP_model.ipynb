{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc359c5-155d-4b6e-b518-572fe3e0143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script collection: limpieza, unificación y entrenamiento para tus CSV de Motive (MoCap)\n",
    "Contiene 4 secciones / scripts (en un solo archivo para que puedas copiarlos separadamente si lo prefieres):\n",
    "\n",
    "1) parse_single_csv(input_path, label, output_path)\n",
    "- Parsea un CSV \"raw\" exportado por Motive (una línea por frame con tokens mixtos)\n",
    "- Extrae solo los marcadores válidos (los de la mano) y rellena con NaN los faltantes\n",
    "- Guarda un csv limpio por take\n",
    "\n",
    "2) process_all(dataset_root, output_folder)\n",
    "- Recorre la estructura de carpetas esperada (/rock/, /paper/, /scissors/)\n",
    "- Llama a parse_single_csv para cada take y concatena todo en mocap_dataset_final.csv\n",
    "\n",
    "3) train_models(mocap_csv_path)\n",
    "- Carga mocap_dataset_final.csv\n",
    "- Imputa NaN (SimpleImputer - mean), estandariza, entrena SVM, RandomForest y XGBoost (si instalado)\n",
    "- Guarda modelos y muestra métricas\n",
    "\n",
    "4) prepare_fusion_example(mocap_csv, rgb_csv)\n",
    "- Ejemplo de cómo preparar la fusión early-level si tienes coincidencia por \"image_file\" o por orden/label\n",
    "\n",
    "USO RÁPIDO (desde terminal):\n",
    "- Para procesar todo: python motive_mocap_processing_and_training.py --process_all /ruta/a/dataset_root /ruta/de/salida\n",
    "- Para entrenar: python motive_mocap_processing_and_training.py --train /ruta/de/salida/mocap_dataset_final.csv\n",
    "\n",
    "Ajustes que puedes modificar en el archivo:\n",
    "- valid_labels: lista de labels exactos que llevarán a columnas\n",
    "- tolerance/handling de tokens\n",
    "- strategy de imputación (ahora usa mean)\n",
    "\n",
    "NOTA: Asumimos que los labels en Motive son exactamente: P1,I1,M1,A1,E1, I2,M2,A2, P2,I3,M3,A3,E2, R,C,U\n",
    "(16 marcadores). Si encuentras diferencias o prefieres otro orden, edita `MARKER_ORDER`.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libs\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Try import xgboost (opcional)\n",
    "try:\n",
    "from xgboost import XGBClassifier\n",
    "XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "XGBOOST_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b9e28-0ed7-4375-8ed4-c419197b4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# CONFIG: marcadores válidos\n",
    "# ---------------------------\n",
    "# Lista exacta de labels esperados en los CSV (según tu confirmación)\n",
    "MARKER_ORDER = [\n",
    "\"P1\",\"I1\",\"M1\",\"A1\",\"E1\", # Distales\n",
    "\"I2\",\"M2\",\"A2\", # PIP\n",
    "\"P2\",\"I3\",\"M3\",\"A3\",\"E2\", # MCP (nudillos)\n",
    "\"R\",\"C\",\"U\" # Muñeca\n",
    "]\n",
    "# Comprobar que tenemos 16 marcadores\n",
    "assert len(MARKER_ORDER) == 16, f\"Expected 16 markers, got {len(MARKER_ORDER)}\"\n",
    "\n",
    "\n",
    "# Column names for final dataframe\n",
    "def make_columns(marker_order: List[str]) -> List[str]:\n",
    "cols = []\n",
    "for m in marker_order:\n",
    "cols += [f\"{m}_x\", f\"{m}_y\", f\"{m}_z\"]\n",
    "return cols\n",
    "\n",
    "\n",
    "FINAL_COLUMNS = [\"label\"] + make_columns(MARKER_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65980b-3d2b-4704-90e2-cf3c174d64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Si token not in marker_order but next token is in marker_order, quizás formato id,label,x,y,z\n",
    "if i+1 < n and tokens[i+1].strip() in marker_order:\n",
    "label = tokens[i+1].strip()\n",
    "try:\n",
    "x = float(tokens[i+2])\n",
    "y = float(tokens[i+3])\n",
    "z = float(tokens[i+4])\n",
    "parsed[f\"{label}_x\"] = x\n",
    "parsed[f\"{label}_y\"] = y\n",
    "parsed[f\"{label}_z\"] = z\n",
    "i += 5\n",
    "continue\n",
    "except Exception:\n",
    "i += 1\n",
    "continue\n",
    "i += 1\n",
    "return parsed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_single_csv(input_path: str, output_path: str, label_value: str = None) -> pd.DataFrame:\n",
    "\"\"\"\n",
    "Lee un CSV exportado por Motive (raw) y devuelve un DataFrame con columnas FINAL_COLUMNS.\n",
    "- input_path: ruta al archivo raw\n",
    "- output_path: si se indica se guarda el csv limpio\n",
    "- label_value: si se indica se añade la columna 'label' con ese valor para todas las filas\n",
    "\"\"\"\n",
    "rows = []\n",
    "with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "reader = csv.reader(f)\n",
    "# Intentamos detectar si el CSV ya está tokenizado en columnas (caso raro)\n",
    "# Si la primera fila tiene muchos campos y contiene alguno de los markers en header, no hacemos parsing raw\n",
    "# Pero: asumimos formato raw como en tus ejemplos: una fila con muchos tokens\n",
    "for raw in reader:\n",
    "# raw is a list resulting from csv.reader splitting by commas\n",
    "# convertir cada celda a string\n",
    "tokens = [str(x).strip() for x in raw if str(x).strip() != \"\"]\n",
    "if len(tokens) == 0:\n",
    "continue\n",
    "# parse line\n",
    "parsed = parse_motive_line_to_dict(tokens, MARKER_ORDER)\n",
    "# construir fila con NaN por defecto\n",
    "row = {c: np.nan for c in FINAL_COLUMNS}\n",
    "if label_value is not None:\n",
    "row['label'] = label_value\n",
    "# completar con valores encontrados\n",
    "for k, v in parsed.items():\n",
    "if k in row:\n",
    "row[k] = v\n",
    "rows.append(row)\n",
    "df = pd.DataFrame(rows, columns=FINAL_COLUMNS)\n",
    "if output_path is not None:\n",
    "df.to_csv(output_path, index=False)\n",
    "return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bcb45-415e-4f42-b02d-5b729a871894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Process all dataset\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def process_all(dataset_root: str, output_folder: str) -> str:\n",
    "\"\"\"\n",
    "Recorre dataset_root/rock, /paper, /scissors. Para cada CSV llama parse_single_csv.\n",
    "Guarda cada cleaned csv en output_folder/cleaned/ y concatena todo en output_folder/mocap_dataset_final.csv\n",
    "Devuelve la ruta al CSV final.\n",
    "\"\"\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "cleaned_dir = os.path.join(output_folder, 'cleaned')\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "final_dfs = []\n",
    "for label in ['rock', 'paper', 'scissors']:\n",
    "folder = os.path.join(dataset_root, label)\n",
    "if not os.path.isdir(folder):\n",
    "print(f\"Warning: folder {folder} not found, saltando\")\n",
    "continue\n",
    "for fname in os.listdir(folder):\n",
    "if not fname.lower().endswith('.csv'):\n",
    "continue\n",
    "inpath = os.path.join(folder, fname)\n",
    "outname = f\"{label}_{os.path.splitext(fname)[0]}_cleaned.csv\"\n",
    "outpath = os.path.join(cleaned_dir, outname)\n",
    "print(f\"Procesando {inpath} -> {outpath}\")\n",
    "df = parse_single_csv(inpath, outpath, label_value=label)\n",
    "final_dfs.append(df)\n",
    "if len(final_dfs) == 0:\n",
    "raise RuntimeError(\"No se procesaron archivos. Revisa la estructura de carpetas o nombres.\")\n",
    "df_all = pd.concat(final_dfs, ignore_index=True)\n",
    "final_csv = os.path.join(output_folder, 'mocap_dataset_final.csv')\n",
    "df_all.to_csv(final_csv, index=False)\n",
    "print(f\"CSV final guardado en: {final_csv}\")\n",
    "return final_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f311bab-56a6-4adb-a00b-a0a83e5a5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Imputar NaN con mean (Option A)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "\n",
    "# Escalar\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X_imp)\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y_enc, test_size=test_size, random_state=random_state, stratify=y_enc)\n",
    "\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='rbf', probability=True, random_state=random_state)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('\\n--- SVM Results ---')\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# RandomForest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=random_state)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print('\\n--- RandomForest Results ---')\n",
    "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "\n",
    "# XGBoost (opcional)\n",
    "if XGBOOST_AVAILABLE:\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_state)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print('\\n--- XGBoost Results ---')\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "\n",
    "# Guardar artefactos\n",
    "os.makedirs(out_models_dir, exist_ok=True)\n",
    "joblib.dump(imputer, os.path.join(out_models_dir, 'imputer.joblib'))\n",
    "joblib.dump(scaler, os.path.join(out_models_dir, 'scaler.joblib'))\n",
    "joblib.dump(le, os.path.join(out_models_dir, 'label_encoder.joblib'))\n",
    "joblib.dump(svm, os.path.join(out_models_dir, 'svm_model.joblib'))\n",
    "joblib.dump(rf, os.path.join(out_models_dir, 'rf_model.joblib'))\n",
    "if XGBOOST_AVAILABLE:\n",
    "joblib.dump(xgb, os.path.join(out_models_dir, 'xgb_model.joblib'))\n",
    "print(f\"Modelos y artefactos guardados en {out_models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1766f7-07ff-45e9-8fad-c637b9e8ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Ejemplo de preparación para fusión con RGB\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def prepare_fusion_example(mocap_csv: str, rgb_csv: str, out_csv: str):\n",
    "\"\"\"\n",
    "Ejemplo simple de fusión: si RGB CSV contiene 'image_file' que incluye un frame id comparable\n",
    "o si ambas tablas comparten la columna 'label' y el mismo número de filas por take, podemos\n",
    "concatenar por orden o join por nombre de archivo.\n",
    "\n",
    "\n",
    "Este es solo un ejemplo: en tu caso lo ideal es tener timestamps y usar merge_asof.\n",
    "\"\"\"\n",
    "df_m = pd.read_csv(mocap_csv)\n",
    "df_r = pd.read_csv(rgb_csv)\n",
    "\n",
    "\n",
    "# Caso A: si ambos tienen 'Time' -> usar merge_asof\n",
    "if 'Time' in df_m.columns and 'Time' in df_r.columns:\n",
    "df_m = df_m.sort_values('Time')\n",
    "df_r = df_r.sort_values('Time')\n",
    "merged = pd.merge_asof(df_r, df_m, on='Time', direction='nearest', tolerance=0.02)\n",
    "merged.to_csv(out_csv, index=False)\n",
    "print('Fusión por Time guardada en', out_csv)\n",
    "return\n",
    "\n",
    "\n",
    "# Caso B: si no hay timestamps, pero los archivos están ordenados y tienen la misma cantidad de frames por take\n",
    "# WARNING: solo válido si estás seguro que las filas corresponden en orden\n",
    "min_len = min(len(df_r), len(df_m))\n",
    "merged = pd.concat([df_r.iloc[:min_len].reset_index(drop=True), df_m.iloc[:min_len].reset_index(drop=True)], axis=1)\n",
    "merged.to_csv(out_csv, index=False)\n",
    "print('Fusión por orden guardada en', out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73a980-c70c-44a7-b077-0251cbc49ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# CLI wrapper\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def main():\n",
    "parser = argparse.ArgumentParser(description='Procesamiento y entrenamiento MoCap -> CSV limpio y modelos')\n",
    "parser.add_argument('--process_all', nargs=2, help='Procesar dataset: <dataset_root> <output_folder>')\n",
    "parser.add_argument('--train', nargs=1, help='Entrenar modelos: <mocap_csv_path>')\n",
    "parser.add_argument('--prepare_fusion', nargs=3, help='Preparar fusión: <mocap_csv> <rgb_csv> <out_csv>')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "if args.process_all:\n",
    "dataset_root, output_folder = args.process_all\n",
    "process_all(dataset_root, output_folder)\n",
    "return\n",
    "if args.train:\n",
    "mocap_csv = args.train[0]\n",
    "train_models(mocap_csv)\n",
    "return\n",
    "if args.prepare_fusion:\n",
    "mocap_csv, rgb_csv, out_csv = args.prepare_fusion\n",
    "prepare_fusion_example(mocap_csv, rgb_csv, out_csv)\n",
    "return\n",
    "\n",
    "\n",
    "parser.print_help()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc46bc3-4f58-4f0d-920e-8a040315a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "Coloca tu dataset con la estructura /dataset_root/rock/*.csv, /dataset_root/paper/*.csv, /dataset_root/scissors/*.csv.\n",
    "\n",
    "Desde la terminal ejecuta (ejemplo):\n",
    "\n",
    "python motive_mocap_processing_and_training.py --process_all /ruta/a/dataset_root /ruta/de/salida\n",
    "\n",
    "luego: python motive_mocap_processing_and_training.py --train /ruta/de/salida/mocap_dataset_final.csv\n",
    "\n",
    "Si necesitas ajustar nombres de markers o la estrategia de imputación, edita la lista MARKER_ORDER y/o la sección de imputación en el script.\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a7392-70a2-4422-9596-e899226cd0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21656-b26c-4ad3-b6bd-ebd0d9ea846f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
